## Digital Human

This project is a digital human that can talk and listen to you. It uses OpenAI's GPT-3 to generate responses, OpenAI's
Whisper to transcript the audio, Eleven Labs to generate voice and Rhubarb Lip Sync to generate the lip sync.

The code performs four main tasks:

* It sets up the environment using the dotenv library to establish the necessary environment variables for interacting with the OpenAI API.

* It defines a "prompt" template using the ChatPromptTemplate class from @langchain/core/prompts. This template guides the conversation as a predefined script for the chat.

* It configures the chat model using the ChatOpenAI class, which relies on OpenAI's "davinci" model if the environment variables have not been configured previously.

* It parses the output, designing the response generated by the AI in a specific format that includes details about the facial expression and animation to use, which is crucial for a realistic interaction with Jack.
  
## How it Operates
The system operates through two primary workflows, depending on whether the user input is in text or audio form:

### Workflow with Text Input:
1. **User Input:** The user enters text.
2. **Text Processing:** The text is forwarded to the OpenAI GPT API for processing.
3. **Audio Generation:** The response from GPT is relayed to the Eleven Labs TTS API to generate audio.
4. **Viseme Generation:** The audio is then sent to Rhubarb Lip Sync to produce viseme metadata.
5. **Synchronization:** The visemes are utilized to synchronize the digital human's lips with the audio.

### Workflow with Audio Input:
1. **User Input:** The user submits audio.
2. **Speech-to-Text Conversion:** The audio is transmitted to the OpenAI Whisper API to convert it into text.
3. **Text Processing:** The converted text is sent to the OpenAI GPT API for further processing.
4. **Audio Generation:** The output from GPT is sent to the Eleven Labs TTS API to produce audio.
5. **Viseme Generation:** The audio is then routed to Rhubarb Lip Sync to generate viseme metadata.
6. **Synchronization:** The visemes are employed to synchronize the digital human's lips with the audio.

<div align="center">
  <img src="resources/architecture.drawio.svg" alt="System Architecture" width="100%">
</div>

## Getting Started

### Requirements
Before using this system, ensure you have the following prerequisites:

1. **OpenAI Subscription:** You must have an active subscription with OpenAI. If you don't have one, you can create it [here](https://openai.com/product).
2. **Eleven Labs Subscription:** You need to have a subscription with Eleven Labs. If you don't have one yet, you can
   sign up [here](https://elevenlabs.io/). 
It's recommended to have the paid version. With the free version, the avatar doesn't work well due to an error caused by too many requests.
3. **Rhubarb Lip-Sync:** Download the latest version of Rhubarb Lip-Sync compatible with your operating system from the
   official. Once downloaded,
   create a `/bin` directory in the backend and move all the contents of the unzipped `rhubarb-lip-sync.zip` into it.
   Sometimes, the operating system requests permissions, so you need to enable them.
4. Install `ffmpeg` for  [Mac OS](https://formulae.brew.sh/formula/ffmpeg), [Linux](https://ffmpeg.org/download.html) or [Windows](https://ffmpeg.org/download.html).

### Installation

1. Clone this repository:
  
```bash
git@github.com:asanchezyali/talking-avatar-with-ai.git
```

2. Navigate to the project directory:

```bash
cd digital-human
```

3. Install dependencies for monorepo:
```bash
yarn
```
4. Create a .env file in the root `/apps/backend/` of the project and add the following environment variables:

```bash
# OPENAI
OPENAI_MODEL=<YOUR_GPT_MODEL>
OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>

# Elevenlabs
ELEVEN_LABS_API_KEY=<YOUR_ELEVEN_LABS_API_KEY>
ELVEN_LABS_VOICE_ID=<YOUR_ELEVEN_LABS_VOICE_ID>
ELEVEN_LABS_MODEL_ID=<YOUR_ELEVEN_LABS_MODEL_ID>
```

5. Run the development system:

```bash
yarn dev
```

6. If you need install another dependence in the monorepo, you can do this:

```bash
yarn add --dev -W <PACKAGE_NAME>
yarn
```


Open [http://localhost:5173/](http://localhost:5173/) with your browser to see the result.
